{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549b1dca",
   "metadata": {},
   "source": [
    "# Llama 3.1 8b 프롬프트, 하이퍼파라미터 튜닝\n",
    "\n",
    "Hugging Face에서 Llama 3.1 8b 토큰 발급받아, Google Cloud Platform의 GPU 가상환경 위에서 실습하였습니다.\n",
    "\n",
    "프롬프트와 하이퍼파라미터를 조금씩 수정해 가며 질문에 따른 답변을 확인했습니다.\n",
    "\n",
    "프롬프트 엔지니어링으로는 따옴표와 기호 등을 이용하거나, 모형에게 역할을 지정(예: 너는 테니스 전문 AI 모형이야)하여, 특정 패턴에 대한 모형의 출력 변화를 확인했습니다.  \n",
    "\n",
    "하이퍼파라미터 튜닝으로는 생성하는 문장의 다양성을 조절하는 temperature, 특정 단어 다음 나올 단어의 확률을 조정하는 top_p, 모형이 가지고 있는 자원 중 답변으로 생성할 최대 자원의 수를 결정하는 max_new_tokens을 조정해 봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7f0a4-8788-464f-8e30-65a0b65e3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"(your huggingface token)\" # set your huggingface token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e6c86-70cf-4627-b001-ae5d8d0fa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "llama_31 = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \n",
    "\n",
    "generator = pipeline(model=llama_31, device=device, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51e455-701c-4744-a487-ae1ff39f2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    generation = generator(\n",
    "        messages,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=30,\n",
    "        max_new_tokens=500\n",
    "    )\n",
    "    \n",
    "    return generation[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb0f5f-5c53-4e2c-a0c6-925c485037d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"text:한글 -> eng  는  한글을 영어로 번역하고 그 번역된 내용의 답변 해달란 의미야. \\\n",
    "text: 머신러닝과 딥러닝의 장단점을 알려줘. -> eng \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5f188-ee72-4371-9bc2-d927927bfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(\"You are a AI assistant who always responds in korean\", user_prompt)\n",
    "ai_message = json.dumps(response, ensure_ascii=False, indent=3)\n",
    "\n",
    "print(f\" Llama3.1 > {ai_message} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e81e04-7b1e-4ee2-9e29-9ee8d17a9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "history = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b98051-2366-4682-a1a3-f005aeeb72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append({\"role\": \"user\", \"content\": \"```다음 문장을 영어로 번역해줘. ```머신러닝 이란 무엇입니까?\"})\n",
    "\n",
    "history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45754861-4946-4031-be9d-ba86b236d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = generator(\n",
    "    history,\n",
    "    do_sample=False,\n",
    "    temperature=0.2,\n",
    "    top_p=60,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "generation[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94320d0-2465-4f91-8e11-dffdc0054b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_user(input_text, chat_history):\n",
    "    \n",
    "    if len(chat_history) == 0:\n",
    "        chat_history.append({\"role\": \"system\", \"content\": \"You are a AI assistant who always responds in korean\"})\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": input_text})\n",
    "\n",
    "    generation = generator(\n",
    "        chat_history,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "\n",
    "    ai_response = generation[0]['generated_text']\n",
    "    \n",
    "    return ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79daa2-f212-4600-a1ae-2c862053a09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    user_prompt = input(\"USER > \")\n",
    "    if user_prompt.lower() == \"quit\":\n",
    "        break\n",
    "        \n",
    "    response = chat_with_user(user_prompt, chat_history)\n",
    "    chat_history = response\n",
    "    ai_message = json.dumps(response, ensure_ascii=False, indent=3)\n",
    "    print(f\" Llama3.1 > {ai_message} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2626da-90b7-4d7f-b798-47ea7acd1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"You are one of the best bilingual AI assistants for tennis coaching. \\\n",
    "You can comprehend both English and Korean. Please answer as Korean.\\\n",
    "테니스 기술을 향상시키는 방법에 대한 일반적인 지식과 전문 지식을 바탕으로, 테니스 기술을 향상시키는 데 필요한 구체적인 지시사항을 제공하는 방법을 알려줘. \\\n",
    "다양한 관점과 시나리오를 고려하여, 테니스 기술을 향상시키는 방법에 대한 명확하고 구체적인 예시와 셈플 데이터를 포함해야 해. \\\n",
    "편향과 민감한 내용에 대한 주의사항을 포함하여, 테니스 기술을 향상시키는 방법에 대한 구체적인 기준이나 체크리스트를 제공해야 해\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3369-db5a-4bd6-9921-cc77fdbb644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(\"You are a AI assistant who always responds in korean\", user_prompt)\n",
    "ai_message = json.dumps(response, ensure_ascii=False, indent=3)\n",
    "\n",
    "print(f\" Llama3.1 > {ai_message} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931fe1b-86d0-4613-a5e5-c107a6812d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \\\n",
    "You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.<|eot_id|><|start_header_id|>user<|end_header_id|>\\\n",
    "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\\n",
    "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\\\n",
    "{\\\n",
    "    \"type\": \"function\",\\\n",
    "    \"function\": {\\\n",
    "    \"name\": \"get_current_conditions\",\\\n",
    "    \"description\": \"Get the current weather conditions for a specific location\",\\\n",
    "    \"parameters\": {\\\n",
    "        \"type\": \"object\",\\\n",
    "        \"properties\": {\\\n",
    "        \"location\": {\\\n",
    "            \"type\": \"string\",\\\n",
    "            \"description\": \"The city and state, e.g., San Francisco, CA\"\\\n",
    "        },\\\n",
    "        \"unit\": {\\\n",
    "            \"type\": \"string\",\\\n",
    "            \"enum\": [\"Celsius\", \"Fahrenheit\"],\\\n",
    "            \"description\": \"The temperature unit to use. Infer this from the user's location.\"\\\n",
    "        }\\\n",
    "        },\\\n",
    "        \"required\": [\"location\", \"unit\"]\\\n",
    "    }\\\n",
    "    }\\\n",
    "}\\\n",
    "\\\n",
    "Question: what is the weather like in San Fransisco?<|eot_id|>>\"\"\"\n",
    "\n",
    "user_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0259c7-a502-4a42-bd9c-6447c80e9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(system_prompt, user_prompt)\n",
    "ai_message = json.dumps(response, ensure_ascii=False, indent=3)\n",
    "\n",
    "print(f\" Llama3.1 > {ai_message} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
